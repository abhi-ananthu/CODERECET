// Import necessary modules from LangChain and Zod
const { z } = require('zod');
const { ChatPromptTemplate } = require('@langchain/core/prompts');
const { StructuredOutputParser } = require('@langchain/core/output_parsers');
const { RunnableSequence } = require('@langchain/core/runnables');
const model = require('./genai');

/**
 * Defines the Zod schema for the desired structured output.
 * Each field includes a .describe() method, which is crucial for
 * the StructuredOutputParser to generate clear instructions for the LLM.
 */
const responseSchema = z.object({
  title: z
    .string()
    .min(1)
    .describe('A concise and descriptive title for the content.'),
  context: z
    .string()
    .min(1)
    .describe('The main textual content or body of the response.'),
  media: z
    .array(
      z.object({
        label: z
          .string()
          .min(1)
          .describe('A descriptive label for the media link.'),
        link: z.string().url().describe('The URL of the media content.'),
      })
    )
    .describe('An array of related media links with labels.'),
  location: z
    .string()
    .min(1)
    .describe('The geographical location relevant to the content.'),
});

/**
 * Creates a LangChain StructuredOutputParser from the defined Zod schema.
 * This parser will take the LLM's raw text output and attempt to transform it
 * into an object conforming to 'responseSchema'.
 */
const parser = StructuredOutputParser.fromZodSchema(responseSchema);

/**
 * Initializes the Generative AI model.
 * You can choose a different modelName (e.g., 'gemini-1.5-pro-latest')
 * and adjust the temperature for creativity vs. strictness.
 */
async function generateStructuredResponse(context) {
  // Get the format instructions generated by the parser.
  // These instructions tell the LLM how to format its output as JSON.
  const formatInstructions = parser.getFormatInstructions();

  // Create a ChatPromptTemplate.
  // - The system message provides instructions to the LLM, including
  //   the crucial 'format_instructions'.
  // - The human message provides the actual context to be analyzed.
  const prompt = ChatPromptTemplate.fromMessages([
    [
      'system',
      'You are an expert assistant designed to extract key information and format it into a structured JSON object. Follow the formatting instructions precisely.\n{format_instructions}',
    ],
    [
      'human',
      'Analyze the following text and extract the requested information:\n\n---\n{context}\n---',
    ],
  ]);

  // Create the RunnableSequence (chain).
  // 1. The prompt is partially filled with format instructions and then
  //    receives the 'context' variable.
  // 2. The prepared prompt is sent to the LLM.
  // 3. The LLM's raw text output is then piped to the parser, which
  //    converts it into the structured JavaScript object.
  const chain = RunnableSequence.from([
    prompt.partial({ format_instructions: formatInstructions }),
    model,
    parser,
  ]);

  try {
    // Invoke the chain with the provided context text.
    const result = await chain.invoke({ context: context });
    return result;
  } catch (error) {
    console.error('Error generating structured response:', error);
    if (error.output) {
      console.error('LLM Raw Output (failed to parse):', error.output);
    }
    throw new Error(
      'Failed to generate structured response. See console for details.'
    );
  }
}

// Export the function for use in other modules
module.exports = { generateStructuredResponse };
